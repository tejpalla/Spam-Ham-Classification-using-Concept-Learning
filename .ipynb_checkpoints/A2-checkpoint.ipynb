{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umXeop1D0A3L",
    "outputId": "09a3af4c-56c2-4088-def1-f988fd8a8502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10']\n",
      "             tree    logreg     Bayes\n",
      "1        0.937093  0.937093  0.811280\n",
      "2        0.900000  0.923913  0.793478\n",
      "3        0.928261  0.952174  0.806522\n",
      "4        0.941304  0.934783  0.821739\n",
      "5        0.930435  0.926087  0.834783\n",
      "6        0.923913  0.921739  0.823913\n",
      "7        0.919565  0.926087  0.808696\n",
      "8        0.891304  0.900000  0.808696\n",
      "9        0.893478  0.928261  0.810870\n",
      "10       0.913043  0.926087  0.810870\n",
      "avg      0.917840  0.927622  0.813085\n",
      "std_dev  0.016933  0.012500  0.010687\n",
      "             tree    logreg     Bayes\n",
      "1        0.054200  0.871700  0.001800\n",
      "2        0.061500  1.045700  0.004200\n",
      "3        0.070400  0.804500  0.005500\n",
      "4        0.062000  1.046700  0.001900\n",
      "5        0.061600  0.935500  0.003500\n",
      "6        0.071400  0.774400  0.001800\n",
      "7        0.063300  0.593100  0.002000\n",
      "8        0.069200  1.242000  0.004300\n",
      "9        0.073500  0.840600  0.002100\n",
      "10       0.058900  0.964400  0.002000\n",
      "avg      0.064600  0.911860  0.002910\n",
      "std_dev  0.005899  0.169448  0.001283\n",
      "             tree    logreg     Bayes\n",
      "1        0.920548  0.919668  0.798144\n",
      "2        0.877660  0.905149  0.785553\n",
      "3        0.908587  0.938889  0.799097\n",
      "4        0.926829  0.916201  0.811060\n",
      "5        0.911111  0.904494  0.820755\n",
      "6        0.903581  0.900552  0.813793\n",
      "7        0.898072  0.900000  0.796296\n",
      "8        0.860335  0.870056  0.794393\n",
      "9        0.868633  0.908587  0.800915\n",
      "10       0.890110  0.904494  0.800915\n",
      "avg      0.896547  0.906809  0.802092\n",
      "std_dev  0.020954  0.016540  0.009795\n",
      "          tree  logreg  Bayes\n",
      "1          1.0     1.0    2.0\n",
      "2          2.0     1.0    3.0\n",
      "3          2.0     1.0    3.0\n",
      "4          1.0     2.0    3.0\n",
      "5          1.0     2.0    3.0\n",
      "6          1.0     2.0    3.0\n",
      "7          2.0     1.0    3.0\n",
      "8          2.0     1.0    3.0\n",
      "9          2.0     1.0    3.0\n",
      "10         2.0     1.0    3.0\n",
      "avg_rank   1.6     1.3    2.9\n",
      "friedman statistic : 14.466666666666663\n",
      "There  is  statistically  significant difference in behaviour between the three models. Null Hypothesis is rejected\n",
      "The critical difference is :1.0478214542564015\n",
      "The performance of Decision Tree and Bayes is not equivalent.\n",
      "The performance of Bayes and Logistic Regression is not equivalent.\n",
      "          tree  logreg  Bayes\n",
      "1          2.0     3.0    1.0\n",
      "2          2.0     3.0    1.0\n",
      "3          2.0     3.0    1.0\n",
      "4          2.0     3.0    1.0\n",
      "5          2.0     3.0    1.0\n",
      "6          2.0     3.0    1.0\n",
      "7          2.0     3.0    1.0\n",
      "8          2.0     3.0    1.0\n",
      "9          2.0     3.0    1.0\n",
      "10         2.0     3.0    1.0\n",
      "avg_rank   2.0     3.0    1.0\n",
      "friedman statistic : 20.0\n",
      "There  is  statistically  significant difference in behaviour between the three models. Null Hypothesis is rejected\n",
      "The critical difference is :1.0478214542564015\n",
      "The performance of Bayes and Logistic Regression is not equivalent.\n",
      "          tree  logreg  Bayes\n",
      "1          1.0     2.0    3.0\n",
      "2          2.0     1.0    3.0\n",
      "3          2.0     1.0    3.0\n",
      "4          1.0     2.0    3.0\n",
      "5          1.0     2.0    3.0\n",
      "6          1.0     2.0    3.0\n",
      "7          2.0     1.0    3.0\n",
      "8          2.0     1.0    3.0\n",
      "9          2.0     1.0    3.0\n",
      "10         2.0     1.0    3.0\n",
      "avg_rank   1.6     1.4    3.0\n",
      "friedman statistic : 15.2\n",
      "There  is  statistically  significant difference in behaviour between the three models. Null Hypothesis is rejected\n",
      "The critical difference is :1.0478214542564015\n",
      "The performance of Decision Tree and Bayes is not equivalent.\n",
      "The performance of Bayes and Logistic Regression is not equivalent.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import math\n",
    "from time import time\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.tree import  DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    " \n",
    "#Loading spambase dataset in dat_file.\n",
    "dat_file = pd.read_csv('spambase.csv')\n",
    "\n",
    "\n",
    "# Splitting output variable from rest data\n",
    "x = dat_file.drop(columns = \"spam\")\n",
    "y = dat_file[\"spam\"]\n",
    "#x = dat_file.iloc[:,:-1]\n",
    "#y = dat_file.iloc[:,-1]\n",
    "\n",
    "\n",
    "#initialization of models from \n",
    "tree = DecisionTreeClassifier()\n",
    "logreg = LogisticRegression(max_iter=5000)\n",
    "Bayes = GaussianNB()\n",
    "\n",
    "\n",
    "#empty lists to hold measures of metrics over 10 folds.lists of metric measures over ten skfolds.\n",
    "train_time_tree=[]\n",
    "#train_time_tree=np.array(train_time_tree)\n",
    "#train_time_logreg=np.array(train_time_logreg)\n",
    "#train_time_Bayes=np.array(train_time_Bayes)\n",
    "train_time_logreg=[]\n",
    "train_time_Bayes=[]\n",
    "\n",
    "acc_tree=[]\n",
    "#acc_tree=np.array(acc_tree)\n",
    "#acc_logreg=np.array(acc_logreg)\n",
    "#acc_Bayes=np.array(acc_Bayes)\n",
    "acc_logreg=[]\n",
    "acc_Bayes=[]\n",
    "\n",
    "f1_tree=[]\n",
    "#f1_tree=np.array(f1_tree)\n",
    "#f1_logreg=np.array(f1_logreg)\n",
    "#f1_Bayes=np.array(f1_Bayes)\n",
    "f1_logreg=[]\n",
    "f1_Bayes=[]\n",
    "\n",
    "\n",
    "#\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "skf.get_n_splits(x, y)    \n",
    "\n",
    "#running 10 fold cross validation\n",
    "for train_index, test_index in skf.split(x, y):\n",
    "    #x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n",
    "    #y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    #print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    x_train = x.iloc[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "    x_test = x.iloc[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    # Decision Tree\n",
    "    begin=time()                             \n",
    "    tree.fit(x_train,y_train)\n",
    "    end=time()                              \n",
    "    t1=round(end-begin,4)                  \n",
    "    y_pred=tree.predict(x_test)       \n",
    "    \n",
    "    acc1=accuracy_score(y_test, y_pred)  \n",
    "    f1_1=f1_score(y_test, y_pred)        \n",
    "    \n",
    "    \n",
    "    train_time_tree.append(t1)\n",
    "    acc_tree.append(acc1)\n",
    "    #train_time_tree = np.append(train_time_tree,t1,axis = 0)\n",
    "    #acc_tree = np.append(acc_tree,acc1,axis = 0)\n",
    "    #f1_tree = np.append(f1_tree,f1_1,axis = 0)\n",
    "    f1_tree.append(f1_1)\n",
    "    \n",
    "    #Logistic regression\n",
    "    begin=time()\n",
    "    logreg.fit(x_train,y_train)\n",
    "    end=time()\n",
    "    t2=round(end-begin,4)\n",
    "    y_pred=logreg.predict(x_test)\n",
    "\n",
    "    acc2=accuracy_score(y_test, y_pred)\n",
    "    f1_2=f1_score(y_test, y_pred)\n",
    "    \n",
    "    train_time_logreg.append(t2)\n",
    "    acc_logreg.append(acc2)\n",
    "    f1_logreg.append(f1_2)\n",
    "    #train_time_logreg = np.append(train_time_logreg,t2,axis = 0)\n",
    "    #acc_logreg = np.append(acc_logreg,acc2,axis = 0)\n",
    "    #f1_logreg = np.append(f1_logreg,f1_2,axis = 0)\n",
    "    \n",
    "    #Bayes\n",
    "    begin=time()\n",
    "    Bayes.fit(x_train,y_train)\n",
    "    end=time()\n",
    "    t3=round(end-begin,4)\n",
    "    y_pred=Bayes.predict(x_test)\n",
    "\n",
    "    acc3=accuracy_score(y_test, y_pred)\n",
    "    f1_3=f1_score(y_test, y_pred)\n",
    "    \n",
    "    train_time_Bayes.append(t3)\n",
    "    acc_Bayes.append(acc3)\n",
    "    f1_Bayes.append(f1_3)\n",
    "    #train_time_Bayes = np.append(train_time_Bayes,t3,axis = 0)\n",
    "    #acc_Bayes = np.append(acc_Bayes,acc3,axis = 0)\n",
    "    #f1_Bayes = np.append(f1_Bayes,f1_3,axis = 0)\n",
    "\n",
    "#converting to numpy arrays and adding mean and std dev at end of array\n",
    "train_time_tree=np.array(train_time_tree)\n",
    "avg=np.mean(train_time_tree)\n",
    "stddev=np.std(train_time_tree)\n",
    "train_time_tree=np.append(train_time_tree,avg)\n",
    "train_time_tree=np.append(train_time_tree,stddev)\n",
    "\n",
    "acc_tree=np.array(acc_tree)\n",
    "avg=np.mean(acc_tree)\n",
    "stddev=np.std(acc_tree)\n",
    "acc_tree=np.append(acc_tree,avg)\n",
    "acc_tree=np.append(acc_tree,stddev)\n",
    "\n",
    "f1_tree=np.array(f1_tree)\n",
    "avg=np.mean(f1_tree)\n",
    "stddev=np.std(f1_tree)\n",
    "f1_tree=np.append(f1_tree,avg)\n",
    "f1_tree=np.append(f1_tree,stddev)\n",
    "\n",
    "train_time_logreg=np.array(train_time_logreg)\n",
    "avg=np.mean(train_time_logreg)\n",
    "stddev=np.std(train_time_logreg)\n",
    "train_time_logreg=np.append(train_time_logreg,avg)\n",
    "train_time_logreg=np.append(train_time_logreg,stddev)\n",
    "\n",
    "acc_logreg=np.array(acc_logreg)\n",
    "avg=np.mean(acc_logreg)\n",
    "stddev=np.std(acc_logreg)\n",
    "acc_logreg=np.append(acc_logreg,avg)\n",
    "acc_logreg=np.append(acc_logreg,stddev)\n",
    "\n",
    "f1_logreg=np.array(f1_logreg)\n",
    "avg=np.mean(f1_logreg)\n",
    "stddev=np.std(f1_logreg)\n",
    "f1_logreg=np.append(f1_logreg,avg)\n",
    "f1_logreg=np.append(f1_logreg,stddev)\n",
    "\n",
    "train_time_Bayes=np.array(train_time_Bayes)\n",
    "avg=np.mean(train_time_Bayes)\n",
    "stddev=np.std(train_time_Bayes)\n",
    "train_time_Bayes=np.append(train_time_Bayes,avg)\n",
    "train_time_Bayes=np.append(train_time_Bayes,stddev)\n",
    "\n",
    "acc_Bayes=np.array(acc_Bayes)\n",
    "avg=np.mean(acc_Bayes)\n",
    "stddev=np.std(acc_Bayes)\n",
    "acc_Bayes=np.append(acc_Bayes,avg)\n",
    "acc_Bayes=np.append(acc_Bayes,stddev)\n",
    "\n",
    "f1_Bayes=np.array(f1_Bayes)\n",
    "avg=np.mean(f1_Bayes)\n",
    "stddev=np.std(f1_Bayes)\n",
    "f1_Bayes=np.append(f1_Bayes,avg)\n",
    "f1_Bayes=np.append(f1_Bayes,stddev)\n",
    "\n",
    "#metrics of decision tree,logistic regression,Naive bayes models are stored in dictionaries.\n",
    "\n",
    "train_time_dict={'tree': train_time_tree ,\n",
    "                 'logreg': train_time_logreg,\n",
    "                 'Bayes': train_time_Bayes }\n",
    "\n",
    "acc_dict={'tree': acc_tree ,\n",
    "          'logreg': acc_logreg,\n",
    "          'Bayes': acc_Bayes }\n",
    "\n",
    "f1_dict={'tree': f1_tree ,\n",
    "         'logreg': f1_logreg, \n",
    "         'Bayes':f1_Bayes }\n",
    "\n",
    "index_array=np.arange(1,11)\n",
    "#index_array=[]\n",
    "#for i in range(1,11):\n",
    "#    index_array.append(str(i))\n",
    "\n",
    "#index_array = list(range(1, 11))\n",
    "#str(index_array)\n",
    "#index_array.append('avg')\n",
    "#index_array.append('std_dev')\n",
    "index_array = index_array.astype(str)\n",
    "print(index_array)\n",
    "index_array = np.append(index_array,('avg','std_dev'))\n",
    "\n",
    "#making dataframes from the recorded metrics and making a rank based version of these dataframes \n",
    "\n",
    "acc_df = pd.DataFrame(acc_dict, index=index_array)\n",
    "print(acc_df)\n",
    "traintime_df = pd.DataFrame(train_time_dict, index=index_array)\n",
    "print(traintime_df)\n",
    "f1_df = pd.DataFrame(f1_dict, index=index_array)\n",
    "print(f1_df)\n",
    "\n",
    "acc_df=acc_df.rank(axis=1,method='dense',ascending=False)\n",
    "acc_df=acc_df.drop(['avg','std_dev'])\n",
    "acc_df.loc['avg_rank']=acc_df.mean()\n",
    "print(acc_df)\n",
    "temp=acc_df.mean(axis=1)\n",
    "avg_rank_acc=temp[-1]\n",
    "\n",
    "temp=acc_df['tree']\n",
    "a1=temp[-1]\n",
    "temp=acc_df['logreg']\n",
    "a2=temp[-1]\n",
    "temp=acc_df['Bayes']\n",
    "a3=temp[-1]\n",
    "\n",
    "\n",
    "#sum squared difference\n",
    "sum_sq=((a1-avg_rank_acc)*(a1-avg_rank_acc)) + ((a2-avg_rank_acc)*(a2-avg_rank_acc)) + ((a3-avg_rank_acc)*(a3-avg_rank_acc))\n",
    "sum_sq=10*sum_sq\n",
    "\n",
    "\n",
    "acc_df=acc_df.drop(['avg_rank'])\n",
    "acc_np=acc_df.to_numpy()\n",
    "temp=acc_np-2\n",
    "temp=np.square(temp)\n",
    "sum_sq2=np.sum(temp)\n",
    "sum_sq2=sum_sq2/20\n",
    "f_no1=sum_sq/sum_sq2\n",
    "\n",
    "print('friedman statistic : '+str(f_no1))\n",
    "\n",
    "crit_val=7.8 #for k=3, n = 10 and alpha = 0.05\n",
    "\n",
    "if f_no1>crit_val:\n",
    "    print(\"There  is  statistically  significant difference in behaviour between the three models. Null Hypothesis is rejected\")\n",
    "else:\n",
    "    print(\"There is no difference in behaviour between three models\")\n",
    "\n",
    "q_alpha=2.343 #from text book\n",
    "k=3\n",
    "n=10\n",
    "alpha=0.05\n",
    "\n",
    "crit_diff=q_alpha*math.sqrt(k*(k+1)/(6*n))\n",
    "\n",
    "print(\"The critical difference is :\" + str(crit_diff))\n",
    "if abs(a1-a2)>crit_diff:\n",
    "    print(\"The performance of Decision Tree and Logistic Regression is not equivalent.\")\n",
    "    \n",
    "if abs(a1-a3)>crit_diff:\n",
    "    print(\"The performance of Decision Tree and Bayes is not equivalent.\")\n",
    "\n",
    "if abs(a2-a3)>crit_diff:\n",
    "    print(\"The performance of Bayes and Logistic Regression is not equivalent.\")\n",
    "\n",
    "traintime_df=traintime_df.rank(axis=1,method='dense',ascending=True)\n",
    "traintime_df=traintime_df.drop(['avg','std_dev'])\n",
    "traintime_df.loc['avg_rank']=traintime_df.mean()\n",
    "print(traintime_df)\n",
    "temp=traintime_df.mean(axis=1)\n",
    "avg_rank_acc=temp[-1]\n",
    "\n",
    "temp=traintime_df['tree']\n",
    "b1=temp[-1]\n",
    "temp=traintime_df['logreg']\n",
    "b2=temp[-1]\n",
    "temp=traintime_df['Bayes']\n",
    "b3=temp[-1]\n",
    "\n",
    "sum_sq=((b1-avg_rank_acc)*(b1-avg_rank_acc)) + ((b2-avg_rank_acc)*(b2-avg_rank_acc)) + ((b3-avg_rank_acc)*(b3-avg_rank_acc))\n",
    "sum_sq=10*sum_sq\n",
    "\n",
    "traintime_df=traintime_df.drop(['avg_rank'])\n",
    "acc_np=traintime_df.to_numpy()\n",
    "temp=acc_np-2\n",
    "temp=np.square(temp)\n",
    "sum_sq2=np.sum(temp)\n",
    "sum_sq2=sum_sq2/20\n",
    "f_no2=sum_sq/sum_sq2\n",
    "\n",
    "print('friedman statistic : '+str(f_no2))\n",
    "\n",
    "crit_val=7.8 #for k=3, n = 10 and alpha = 0.05\n",
    "\n",
    "if f_no2>crit_val:\n",
    "    print(\"There  is  statistically  significant difference in behaviour between the three models. Null Hypothesis is rejected\")\n",
    "else:\n",
    "    print(\"There is no difference in behaviour between three models\")\n",
    "\n",
    "q_alpha=2.343\n",
    "k=3\n",
    "n=10\n",
    "alpha=0.05\n",
    "\n",
    "crit_diff=q_alpha*math.sqrt(k*(k+1)/(6*n))\n",
    "\n",
    "print(\"The critical difference is :\" + str(crit_diff))\n",
    "if abs(b1-b2)>crit_diff:\n",
    "    print(\"The performance of Decision Tree and Logistic Regression is not equivalent.\")\n",
    "\n",
    "if abs(b1-b3)>crit_diff:\n",
    "    print(\"The performance of Decision Tree and Bayes is not equivalent.\")\n",
    "\n",
    "if abs(b2-b3)>crit_diff:\n",
    "    print(\"The performance of Bayes and Logistic Regression is not equivalent.\")\n",
    "\n",
    "\n",
    "f1_df=f1_df.rank(axis=1,method='dense',ascending=False)\n",
    "f1_df=f1_df.drop(['avg','std_dev'])\n",
    "f1_df.loc['avg_rank']=f1_df.mean()\n",
    "print(f1_df)\n",
    "temp=f1_df.mean(axis=1)\n",
    "avg_rank_acc=temp[-1]\n",
    "\n",
    "temp=f1_df['tree']\n",
    "c1=temp[-1]\n",
    "temp=f1_df['logreg']\n",
    "c2=temp[-1]\n",
    "temp=f1_df['Bayes']\n",
    "c3=temp[-1]\n",
    "\n",
    "sum_sq=((c1-avg_rank_acc)*(c1-avg_rank_acc)) + ((c2-avg_rank_acc)*(c2-avg_rank_acc)) + ((c3-avg_rank_acc)*(c3-avg_rank_acc))\n",
    "sum_sq=10*sum_sq\n",
    "\n",
    "f1_df=f1_df.drop(['avg_rank'])\n",
    "acc_np=f1_df.to_numpy()\n",
    "temp=acc_np-2\n",
    "temp=np.square(temp)\n",
    "sum_sq2=np.sum(temp)\n",
    "sum_sq2=sum_sq2/20\n",
    "\n",
    "f_no3=sum_sq/sum_sq2\n",
    "\n",
    "print('friedman statistic : '+str(f_no3))\n",
    "\n",
    "crit_val=7.8 #for k=3, n = 10 and alpha = 0.05\n",
    "\n",
    "if f_no3>crit_val:\n",
    "    print(\"There  is  statistically  significant difference in behaviour between the three models. Null Hypothesis is rejected\")\n",
    "else:\n",
    "    print(\"There is no difference in behaviour between three models\")\n",
    "\n",
    "q_alpha=2.343\n",
    "k=3\n",
    "n=10\n",
    "alpha=0.05\n",
    "\n",
    "crit_diff=q_alpha*math.sqrt(k*(k+1)/(6*n))\n",
    "\n",
    "print(\"The critical difference is :\" + str(crit_diff))\n",
    "if abs(c1-c2)>crit_diff:\n",
    "    print(\"The performance of Decision Tree and Logistic Regression is not equivalent.\")\n",
    "\n",
    "    \n",
    "if abs(c1-c3)>crit_diff:\n",
    "    print(\"The performance of Decision Tree and Bayes is not equivalent.\")\n",
    "\n",
    "\n",
    "if abs(c2-c3)>crit_diff:\n",
    "    print(\"The performance of Bayes and Logistic Regression is not equivalent.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "Jg4LFYeD0A3l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 'tej']\n",
      "[1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "tej = list(range(1,10))\n",
    "\n",
    "tej.append('tej')\n",
    "print(tej)\n",
    "print(index_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    tree  logreg  Bayes\n",
      "1    1.0     2.0    3.0\n",
      "2    2.0     1.0    3.0\n",
      "3    2.0     1.0    3.0\n",
      "4    2.0     1.0    3.0\n",
      "5    2.0     1.0    3.0\n",
      "6    2.0     1.0    3.0\n",
      "7    2.0     1.0    3.0\n",
      "8    2.0     1.0    3.0\n",
      "9    1.0     2.0    3.0\n",
      "10   2.0     1.0    3.0\n"
     ]
    }
   ],
   "source": [
    "print(f1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     3.0\n",
      "2     3.0\n",
      "3     3.0\n",
      "4     3.0\n",
      "5     3.0\n",
      "6     3.0\n",
      "7     3.0\n",
      "8     3.0\n",
      "9     3.0\n",
      "10    3.0\n",
      "Name: Bayes, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "temp=f1_df['Bayes']\n",
    "c3=temp[-1]\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi = np.arange(1,11)\n",
    "hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' 'avg' 'std_dev' 'std_dev']\n",
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' 'avg' 'std_dev']\n"
     ]
    }
   ],
   "source": [
    "index_array=np.arange(1,11)\n",
    "#index_array=[]\n",
    "#for i in range(1,11):\n",
    "#    index_array.append(str(i))\n",
    "\n",
    "#index_array = list(range(1, 11))\n",
    "#str(index_array)\n",
    "#index_array.append('avg')\n",
    "#index_array.append('std_dev')\n",
    "index_array= index_array.astype(str)\n",
    "print(res)\n",
    "index_array = np.append(index_array,('avg','std_dev'))\n",
    "\n",
    "print(index_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.213015</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.064769</td>\n",
       "      <td>0.312223</td>\n",
       "      <td>0.095901</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>5.191515</td>\n",
       "      <td>52.172789</td>\n",
       "      <td>283.289285</td>\n",
       "      <td>0.394045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.305358</td>\n",
       "      <td>1.290575</td>\n",
       "      <td>0.504143</td>\n",
       "      <td>1.392893</td>\n",
       "      <td>0.672513</td>\n",
       "      <td>0.273824</td>\n",
       "      <td>0.391441</td>\n",
       "      <td>0.401071</td>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.644755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.109394</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.429342</td>\n",
       "      <td>31.729449</td>\n",
       "      <td>194.891310</td>\n",
       "      <td>606.347851</td>\n",
       "      <td>0.488698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.276000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.706000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.540000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.385000</td>\n",
       "      <td>9.752000</td>\n",
       "      <td>4.081000</td>\n",
       "      <td>32.478000</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.829000</td>\n",
       "      <td>1102.500000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>15841.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "count     4601.000000        4601.000000    4601.000000   4601.000000   \n",
       "mean         0.104553           0.213015       0.280656      0.064769   \n",
       "std          0.305358           1.290575       0.504143      1.392893   \n",
       "min          0.000000           0.000000       0.000000      0.000000   \n",
       "25%          0.000000           0.000000       0.000000      0.000000   \n",
       "50%          0.000000           0.000000       0.000000      0.000000   \n",
       "75%          0.000000           0.000000       0.420000      0.000000   \n",
       "max          4.540000          14.280000       5.100000     43.000000   \n",
       "\n",
       "       word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "count    4601.000000     4601.000000       4601.000000         4601.000000   \n",
       "mean        0.312223        0.095901          0.114208            0.105295   \n",
       "std         0.672513        0.273824          0.391441            0.401071   \n",
       "min         0.000000        0.000000          0.000000            0.000000   \n",
       "25%         0.000000        0.000000          0.000000            0.000000   \n",
       "50%         0.000000        0.000000          0.000000            0.000000   \n",
       "75%         0.380000        0.000000          0.000000            0.000000   \n",
       "max        10.000000        5.880000          7.270000           11.110000   \n",
       "\n",
       "       word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "count      4601.000000     4601.000000  ...  4601.000000  4601.000000   \n",
       "mean          0.090067        0.239413  ...     0.038575     0.139030   \n",
       "std           0.278616        0.644755  ...     0.243471     0.270355   \n",
       "min           0.000000        0.000000  ...     0.000000     0.000000   \n",
       "25%           0.000000        0.000000  ...     0.000000     0.000000   \n",
       "50%           0.000000        0.000000  ...     0.000000     0.065000   \n",
       "75%           0.000000        0.160000  ...     0.000000     0.188000   \n",
       "max           5.260000       18.180000  ...     4.385000     9.752000   \n",
       "\n",
       "       char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.016976     0.269071     0.075811     0.044238   \n",
       "std       0.109394     0.815672     0.245882     0.429342   \n",
       "min       0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.315000     0.052000     0.000000   \n",
       "max       4.081000    32.478000     6.003000    19.829000   \n",
       "\n",
       "       capital_run_length_average  capital_run_length_longest  \\\n",
       "count                 4601.000000                 4601.000000   \n",
       "mean                     5.191515                   52.172789   \n",
       "std                     31.729449                  194.891310   \n",
       "min                      1.000000                    1.000000   \n",
       "25%                      1.588000                    6.000000   \n",
       "50%                      2.276000                   15.000000   \n",
       "75%                      3.706000                   43.000000   \n",
       "max                   1102.500000                 9989.000000   \n",
       "\n",
       "       capital_run_length_total         spam  \n",
       "count               4601.000000  4601.000000  \n",
       "mean                 283.289285     0.394045  \n",
       "std                  606.347851     0.488698  \n",
       "min                    1.000000     0.000000  \n",
       "25%                   35.000000     0.000000  \n",
       "50%                   95.000000     0.000000  \n",
       "75%                  266.000000     1.000000  \n",
       "max                15841.000000     1.000000  \n",
       "\n",
       "[8 rows x 58 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_file.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "4596    0\n",
       "4597    0\n",
       "4598    0\n",
       "4599    0\n",
       "4600    0\n",
       "Name: spam, Length: 4601, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "4596    0\n",
       "4597    0\n",
       "4598    0\n",
       "4599    0\n",
       "4600    0\n",
       "Name: spam, Length: 4601, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "MLA2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
