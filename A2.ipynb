{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umXeop1D0A3L",
    "outputId": "09a3af4c-56c2-4088-def1-f988fd8a8502"
   },
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import math\n",
    "from time import time\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.tree import  DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umXeop1D0A3L",
    "outputId": "09a3af4c-56c2-4088-def1-f988fd8a8502"
   },
   "outputs": [],
   "source": [
    "#Loading spambase dataset in dat_file.\n",
    "dat_file = pd.read_csv('spambase.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umXeop1D0A3L",
    "outputId": "09a3af4c-56c2-4088-def1-f988fd8a8502"
   },
   "outputs": [],
   "source": [
    "# Splitting output variable from rest data\n",
    "x = dat_file.drop(columns = \"spam\")\n",
    "y = dat_file[\"spam\"]\n",
    "#x = dat_file.iloc[:,:-1]\n",
    "#y = dat_file.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umXeop1D0A3L",
    "outputId": "09a3af4c-56c2-4088-def1-f988fd8a8502"
   },
   "outputs": [],
   "source": [
    "#initialization of models \n",
    "tree = DecisionTreeClassifier()\n",
    "logreg = LogisticRegression(max_iter=5000)\n",
    "Bayes = GaussianNB()\n",
    "\n",
    "#lists of metric measures over ten skfolds.\n",
    "TT_tree=[]\n",
    "TT_logreg=[]\n",
    "TT_Bayes=[]\n",
    "\n",
    "Acc_tree=[]\n",
    "Acc_logreg=[]\n",
    "Acc_Bayes=[]\n",
    "\n",
    "F1_tree=[]\n",
    "F1_logreg=[]\n",
    "F1_Bayes=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umXeop1D0A3L",
    "outputId": "09a3af4c-56c2-4088-def1-f988fd8a8502"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initializing stratifiedkfold and shuffle is set to true for randomizing the samples of each fold for each class.\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "skf.get_n_splits(x, y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umXeop1D0A3L",
    "outputId": "09a3af4c-56c2-4088-def1-f988fd8a8502"
   },
   "outputs": [],
   "source": [
    "#running 10 fold cross validation\n",
    "for train_index, test_index in skf.split(x, y):\n",
    "    #print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    x_train = x.iloc[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "    x_test = x.iloc[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    # Evaluating model's training time,accuracy and f1_score of current fold.\n",
    "    #Decision tree\n",
    "    begin=time()                             \n",
    "    tree.fit(x_train,y_train)\n",
    "    end=time()                              \n",
    "    t1=round(end-begin,4)                  \n",
    "    y_pred=tree.predict(x_test)       \n",
    "    acc1=accuracy_score(y_test, y_pred)  \n",
    "    f1_1=f1_score(y_test, y_pred)  \n",
    "    \n",
    "    # logistic regression\n",
    "    begin=time()\n",
    "    logreg.fit(x_train,y_train)\n",
    "    end=time()\n",
    "    t2=round(end-begin,4)\n",
    "    y_pred=logreg.predict(x_test)\n",
    "    acc2=accuracy_score(y_test, y_pred)\n",
    "    f1_2=f1_score(y_test, y_pred)\n",
    "    \n",
    "    #Bayes\n",
    "    begin=time()\n",
    "    Bayes.fit(x_train,y_train)\n",
    "    end=time()\n",
    "    t3=round(end-begin,4)\n",
    "    y_pred=Bayes.predict(x_test)\n",
    "    acc3=accuracy_score(y_test, y_pred)\n",
    "    f1_3=f1_score(y_test, y_pred)\n",
    "    \n",
    "    \n",
    "    #loading the output in list of metric measures.\n",
    "    TT_tree.append(t1)\n",
    "    Acc_tree.append(acc1)\n",
    "    F1_tree.append(f1_1)\n",
    "    \n",
    "    TT_logreg.append(t2)\n",
    "    Acc_logreg.append(acc2)\n",
    "    F1_logreg.append(f1_2)\n",
    "\n",
    "    TT_Bayes.append(t3)\n",
    "    Acc_Bayes.append(acc3)\n",
    "    F1_Bayes.append(f1_3)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umXeop1D0A3L",
    "outputId": "09a3af4c-56c2-4088-def1-f988fd8a8502"
   },
   "outputs": [],
   "source": [
    "#lists of metric measures are converted to numpy arrays.\n",
    "TT_tree=np.array(TT_tree)\n",
    "Acc_tree=np.array(Acc_tree)\n",
    "F1_tree=np.array(F1_tree)\n",
    "\n",
    "TT_logreg=np.array(TT_logreg)\n",
    "Acc_logreg=np.array(Acc_logreg)\n",
    "F1_logreg=np.array(F1_logreg)\n",
    "\n",
    "TT_Bayes=np.array(TT_Bayes)\n",
    "Acc_Bayes=np.array(Acc_Bayes)\n",
    "F1_Bayes=np.array(F1_Bayes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umXeop1D0A3L",
    "outputId": "09a3af4c-56c2-4088-def1-f988fd8a8502"
   },
   "outputs": [],
   "source": [
    "#calculating mean and standard deviation of metric measures using numpy.\n",
    "avg=np.mean(TT_tree)\n",
    "stddev=np.std(TT_tree)\n",
    "\n",
    "avg=np.mean(Acc_tree)\n",
    "stddev=np.std(Acc_tree)\n",
    "\n",
    "avg=np.mean(F1_tree)\n",
    "stddev=np.std(F1_tree)\n",
    "\n",
    "avg=np.mean(TT_logreg)\n",
    "stddev=np.std(TT_logreg)\n",
    "\n",
    "avg=np.mean(Acc_logreg)\n",
    "stddev=np.std(Acc_logreg)\n",
    "\n",
    "avg=np.mean(F1_logreg)\n",
    "stddev=np.std(F1_logreg)\n",
    "\n",
    "avg=np.mean(TT_Bayes)\n",
    "stddev=np.std(TT_Bayes)\n",
    "\n",
    "avg=np.mean(Acc_Bayes)\n",
    "stddev=np.std(Acc_Bayes)\n",
    "\n",
    "avg=np.mean(F1_Bayes)\n",
    "stddev=np.std(F1_Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umXeop1D0A3L",
    "outputId": "09a3af4c-56c2-4088-def1-f988fd8a8502"
   },
   "outputs": [],
   "source": [
    "#appending avg and standard deviation values to numpy arrays.\n",
    "TT_tree=np.append(TT_tree,avg)\n",
    "TT_tree=np.append(TT_tree,stddev)\n",
    "\n",
    "Acc_tree=np.append(Acc_tree,avg)\n",
    "Acc_tree=np.append(Acc_tree,stddev)\n",
    "\n",
    "F1_tree=np.append(F1_tree,avg)\n",
    "F1_tree=np.append(F1_tree,stddev)\n",
    "\n",
    "TT_logreg=np.append(TT_logreg,avg)\n",
    "TT_logreg=np.append(TT_logreg,stddev)\n",
    "\n",
    "Acc_logreg=np.append(Acc_logreg,avg)\n",
    "Acc_logreg=np.append(Acc_logreg,stddev)\n",
    "\n",
    "F1_logreg=np.append(F1_logreg,avg)\n",
    "F1_logreg=np.append(F1_logreg,stddev)\n",
    "\n",
    "TT_Bayes=np.append(TT_Bayes,avg)\n",
    "TT_Bayes=np.append(TT_Bayes,stddev)\n",
    "\n",
    "Acc_Bayes=np.append(Acc_Bayes,avg)\n",
    "Acc_Bayes=np.append(Acc_Bayes,stddev)\n",
    "\n",
    "F1_Bayes=np.append(F1_Bayes,avg)\n",
    "F1_Bayes=np.append(F1_Bayes,stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umXeop1D0A3L",
    "outputId": "09a3af4c-56c2-4088-def1-f988fd8a8502"
   },
   "outputs": [],
   "source": [
    "#Creating dictionaries to store metric measures of decision tree,logistic regression,Naive Bayes.\n",
    "\n",
    "TT_dict={'Decision Tree': TT_tree ,\n",
    "         'logistic regression': TT_logreg,\n",
    "         'Bayes': TT_Bayes }\n",
    "\n",
    "Acc_dict={'Decision Tree': Acc_tree ,\n",
    "          'logistic regression': Acc_logreg,\n",
    "          'Bayes': Acc_Bayes }\n",
    "\n",
    "F1_dict={'Decision Tree': F1_tree ,\n",
    "         'logistic regression': F1_logreg, \n",
    "         'Bayes':F1_Bayes }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umXeop1D0A3L",
    "outputId": "09a3af4c-56c2-4088-def1-f988fd8a8502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Decision Tree  logistic regression     Bayes\n",
      "1             0.876712             0.917127  0.788155\n",
      "2             0.910995             0.901961  0.791762\n",
      "3             0.882192             0.932584  0.802752\n",
      "4             0.901961             0.905556  0.769231\n",
      "5             0.895604             0.899713  0.820276\n",
      "6             0.900000             0.901408  0.825472\n",
      "7             0.916890             0.885057  0.790698\n",
      "8             0.885870             0.929972  0.801865\n",
      "9             0.871935             0.857143  0.807339\n",
      "10            0.898630             0.924791  0.806452\n",
      "avg           0.800400             0.800400  0.800400\n",
      "std_dev       0.015512             0.015512  0.015512\n",
      "         Decision Tree  logistic regression     Bayes\n",
      "1             0.902386             0.934924  0.798265\n",
      "2             0.926087             0.923913  0.802174\n",
      "3             0.906522             0.947826  0.813043\n",
      "4             0.923913             0.926087  0.778261\n",
      "5             0.917391             0.923913  0.830435\n",
      "6             0.921739             0.923913  0.839130\n",
      "7             0.932609             0.913043  0.804348\n",
      "8             0.908696             0.945652  0.815217\n",
      "9             0.897826             0.886957  0.817391\n",
      "10            0.919565             0.941304  0.817391\n",
      "avg           0.800400             0.800400  0.800400\n",
      "std_dev       0.015512             0.015512  0.015512\n",
      "         Decision Tree  logistic regression     Bayes\n",
      "1             0.057200             0.965700  0.002000\n",
      "2             0.063400             0.863700  0.004900\n",
      "3             0.068700             0.875300  0.002100\n",
      "4             0.076500             0.899200  0.001900\n",
      "5             0.060400             0.889200  0.002000\n",
      "6             0.064000             0.922800  0.005200\n",
      "7             0.065700             1.345600  0.003800\n",
      "8             0.066400             1.553400  0.002000\n",
      "9             0.071000             1.448900  0.001900\n",
      "10            0.060100             1.949500  0.002000\n",
      "avg           0.800400             0.800400  0.800400\n",
      "std_dev       0.015512             0.015512  0.015512\n"
     ]
    }
   ],
   "source": [
    "index_array = np.arange(1,11)\n",
    "index_array = index_array.astype(str)\n",
    "index_array = np.append(index_array,('avg','std_dev'))\n",
    "\n",
    "#making dataframes from the recorded metrics and making a rank based version of these dataframes.\n",
    "\n",
    "Acc_df = pd.DataFrame(Acc_dict, index = index_array)\n",
    "TT_df = pd.DataFrame(TT_dict, index = index_array)\n",
    "F1_df = pd.DataFrame(F1_dict, index = index_array)\n",
    "\n",
    "print(F1_df)\n",
    "print(Acc_df)\n",
    "print(TT_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umXeop1D0A3L",
    "outputId": "09a3af4c-56c2-4088-def1-f988fd8a8502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Decision Tree  logistic regression  Bayes\n",
      "1                   2.0                  1.0    3.0\n",
      "2                   1.0                  2.0    3.0\n",
      "3                   2.0                  1.0    3.0\n",
      "4                   2.0                  1.0    3.0\n",
      "5                   2.0                  1.0    3.0\n",
      "6                   2.0                  1.0    3.0\n",
      "7                   1.0                  2.0    3.0\n",
      "8                   2.0                  1.0    3.0\n",
      "9                   1.0                  2.0    3.0\n",
      "10                  2.0                  1.0    3.0\n",
      "avg_rank            1.7                  1.3    3.0\n",
      "friedman statistic : 15.8\n",
      "There  is  statistically  significant difference in behaviour between the three models. Null Hypothesis is rejected\n",
      "The critical difference is :1.0478214542564015\n",
      "The performance of Decision Tree and Bayes is not equivalent.\n",
      "The performance of Bayes and Logistic Regression is not equivalent.\n"
     ]
    }
   ],
   "source": [
    "# Conducting friedman test and nemeyi test for F1 score of selected models.\n",
    "F1_df = F1_df.rank(axis=1,method='dense',ascending=False)\n",
    "F1_df = F1_df.drop(['avg','std_dev'])\n",
    "F1_df.loc['avg_rank']=F1_df.mean()\n",
    "print(F1_df)\n",
    "avg_rank_acc = F1_df.mean(axis=1)[-1]\n",
    "\n",
    "c1 = F1_df['Decision Tree'][-1]\n",
    "c2 = F1_df['logistic regression'][-1]\n",
    "c3 = F1_df['Bayes'][-1]\n",
    "\n",
    "c = np.array([c1,c2,c3])\n",
    "c = c - avg_rank_acc\n",
    "sum_square = 10 * np.sum(np.square(c))\n",
    "\n",
    "#sum_square = 10*((c1-avg_rank_acc)**2 + (c2-avg_rank_acc)**2 + (c3-avg_rank_acc)**2)\n",
    "F1_df = F1_df.drop(['avg_rank'])\n",
    "acc_np = F1_df.to_numpy()\n",
    "sum_square2 = np.sum(np.square(acc_np-2))/20\n",
    "f_stat = sum_square/sum_square2\n",
    "\n",
    "\n",
    "print('friedman statistic : '+str(f_stat))\n",
    "\n",
    "crit_val = 7.8 #for k=3, n = 10 and alpha = 0.05\n",
    "\n",
    "if f_stat>crit_val:\n",
    "    print(\"There  is  statistically  significant difference in behaviour between the three models. Null Hypothesis is rejected\")\n",
    "else:\n",
    "    print(\"There is no difference in behaviour between three models\")\n",
    "\n",
    "q_alpha = 2.343\n",
    "k = 3\n",
    "n = 10\n",
    "alpha = 0.05\n",
    "\n",
    "crit_diff = q_alpha*math.sqrt(k*(k+1)/(6*n))\n",
    "\n",
    "print(\"The critical difference is :\" + str(crit_diff))\n",
    "if abs(c1-c2) > crit_diff:\n",
    "    print(\"The performance of Decision Tree and Logistic Regression is not equivalent.\")\n",
    "\n",
    "    \n",
    "if abs(c1-c3) > crit_diff:\n",
    "    print(\"The performance of Decision Tree and Bayes is not equivalent.\")\n",
    "\n",
    "\n",
    "if abs(c2-c3) > crit_diff:\n",
    "    print(\"The performance of Bayes and Logistic Regression is not equivalent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umXeop1D0A3L",
    "outputId": "09a3af4c-56c2-4088-def1-f988fd8a8502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Decision Tree  logistic regression  Bayes\n",
      "1                   2.0                  1.0    3.0\n",
      "2                   1.0                  2.0    3.0\n",
      "3                   2.0                  1.0    3.0\n",
      "4                   2.0                  1.0    3.0\n",
      "5                   2.0                  1.0    3.0\n",
      "6                   2.0                  1.0    3.0\n",
      "7                   1.0                  2.0    3.0\n",
      "8                   2.0                  1.0    3.0\n",
      "9                   1.0                  2.0    3.0\n",
      "10                  2.0                  1.0    3.0\n",
      "avg_rank            1.7                  1.3    3.0\n",
      "friedman statistic : 15.8\n",
      "There  is  statistically  significant difference in behaviour between the three models. Null Hypothesis is rejected\n",
      "The critical difference is :1.0478214542564015\n",
      "The performance of Decision Tree and Bayes is not equivalent.\n",
      "The performance of Bayes and Logistic Regression is not equivalent.\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "# Conducting friedman test and nemeyi test for Accuracy of selected models \n",
    "#Friedman test\n",
    "Acc_df = Acc_df.rank(axis=1,method='dense',ascending=False)\n",
    "Acc_df = Acc_df.drop(['avg','std_dev'])\n",
    "Acc_df.loc['avg_rank']=Acc_df.mean()\n",
    "print(Acc_df)\n",
    "avg_rank_acc = Acc_df.mean(axis=1)[-1]\n",
    "\n",
    "\n",
    "a1 = Acc_df['Decision Tree'][-1]\n",
    "a2 = Acc_df['logistic regression'][-1]\n",
    "a3 = Acc_df['Bayes'][-1]\n",
    "\n",
    "\n",
    "#sum of squared difference\n",
    "a = np.array([a1,a2,a3])\n",
    "a = a - avg_rank_acc\n",
    "sum_square = 10 * np.sum(np.square(a))\n",
    "#sum_square = 10*((a1-avg_rank_acc)**2 + (a2-avg_rank_acc)**2 + (a3-avg_rank_acc)**2)\n",
    "\n",
    "\n",
    "Acc_df = Acc_df.drop(['avg_rank'])\n",
    "acc_np = Acc_df.to_numpy() \n",
    "sum_square2 = np.sum(np.square(acc_np-2))/20\n",
    "\n",
    "f_stat = sum_square/sum_square2\n",
    "\n",
    "\n",
    "print('friedman statistic : '+str(f_stat))\n",
    "\n",
    "crit_val = 7.8 #for k=3, n = 10 and alpha = 0.05\n",
    "\n",
    "if f_stat > crit_val:\n",
    "    print(\"There  is  statistically  significant difference in behaviour between the three models. Null Hypothesis is rejected\")\n",
    "else:\n",
    "    print(\"There is no difference in behaviour between three models\")\n",
    "\n",
    "    \n",
    "# Nemeyi test\n",
    "q_alpha = 2.343 #from text book\n",
    "k = 3\n",
    "n = 10\n",
    "alpha = 0.05\n",
    "\n",
    "crit_diff = q_alpha * math.sqrt(k*(k+1)/(6*n))\n",
    "\n",
    "print(\"The critical difference is :\" + str(crit_diff))\n",
    "if abs(a1-a2)>crit_diff:\n",
    "    print(\"The performance of Decision Tree and Logistic Regression is not equivalent.\")\n",
    "    \n",
    "if abs(a1-a3)>crit_diff:\n",
    "    print(\"The performance of Decision Tree and Bayes is not equivalent.\")\n",
    "\n",
    "if abs(a2-a3)>crit_diff:\n",
    "    print(\"The performance of Bayes and Logistic Regression is not equivalent.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umXeop1D0A3L",
    "outputId": "09a3af4c-56c2-4088-def1-f988fd8a8502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Decision Tree  logistic regression  Bayes\n",
      "1                   2.0                  3.0    1.0\n",
      "2                   2.0                  3.0    1.0\n",
      "3                   2.0                  3.0    1.0\n",
      "4                   2.0                  3.0    1.0\n",
      "5                   2.0                  3.0    1.0\n",
      "6                   2.0                  3.0    1.0\n",
      "7                   2.0                  3.0    1.0\n",
      "8                   2.0                  3.0    1.0\n",
      "9                   2.0                  3.0    1.0\n",
      "10                  2.0                  3.0    1.0\n",
      "avg_rank            2.0                  3.0    1.0\n",
      "friedman statistic : 20.0\n",
      "There  is  statistically  significant difference in behaviour between the three models. Null Hypothesis is rejected\n",
      "The critical difference is :1.0478214542564015\n",
      "The performance of Bayes and Logistic Regression is not equivalent.\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Conducting friedman test and nemeyi test for training time of selected models \n",
    "TT_df = TT_df.rank(axis=1,method='dense',ascending=True)\n",
    "TT_df = TT_df.drop(['avg','std_dev'])\n",
    "TT_df.loc['avg_rank']=TT_df.mean()\n",
    "print(TT_df)\n",
    "avg_rank_acc = TT_df.mean(axis=1)[-1]\n",
    "\n",
    "\n",
    "b1 = TT_df['Decision Tree'][-1]\n",
    "b2 = TT_df['logistic regression'][-1]\n",
    "b3 = TT_df['Bayes'][-1]\n",
    "\n",
    "b = np.array([b1,b2,b3])\n",
    "b = b - avg_rank_acc\n",
    "sum_square = 10 * np.sum(np.square(b))\n",
    "#sum_square = 10*((b1-avg_rank_acc)**2 + (b2-avg_rank_acc)**2 + (b3-avg_rank_acc)**2)\n",
    "\n",
    "TT_df = TT_df.drop(['avg_rank'])\n",
    "acc_np = TT_df.to_numpy()\n",
    "sum_square2 = np.sum(np.square(acc_np-2))/20\n",
    "f_stat = sum_square/sum_square2\n",
    "\n",
    "\n",
    "print('friedman statistic : '+str(f_stat))\n",
    "\n",
    "crit_val = 7.8 #for k=3, n = 10 and alpha = 0.05\n",
    "\n",
    "if f_stat > crit_val:\n",
    "    print(\"There  is  statistically  significant difference in behaviour between the three models. Null Hypothesis is rejected\")\n",
    "else:\n",
    "    print(\"There is no difference in behaviour between three models\")\n",
    "\n",
    "# Nemeyi test\n",
    "q_alpha = 2.343\n",
    "k = 3\n",
    "n = 10\n",
    "alpha = 0.05\n",
    "\n",
    "crit_diff = q_alpha * math.sqrt(k*(k+1)/(6*n))\n",
    "\n",
    "print(\"The critical difference is :\" + str(crit_diff))\n",
    "if abs(b1-b2)>crit_diff:\n",
    "    print(\"The performance of Decision Tree and Logistic Regression is not equivalent.\")\n",
    "\n",
    "if abs(b1-b3)>crit_diff:\n",
    "    print(\"The performance of Decision Tree and Bayes is not equivalent.\")\n",
    "\n",
    "if abs(b2-b3)>crit_diff:\n",
    "    print(\"The performance of Bayes and Logistic Regression is not equivalent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.213015</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.064769</td>\n",
       "      <td>0.312223</td>\n",
       "      <td>0.095901</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>5.191515</td>\n",
       "      <td>52.172789</td>\n",
       "      <td>283.289285</td>\n",
       "      <td>0.394045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.305358</td>\n",
       "      <td>1.290575</td>\n",
       "      <td>0.504143</td>\n",
       "      <td>1.392893</td>\n",
       "      <td>0.672513</td>\n",
       "      <td>0.273824</td>\n",
       "      <td>0.391441</td>\n",
       "      <td>0.401071</td>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.644755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.109394</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.429342</td>\n",
       "      <td>31.729449</td>\n",
       "      <td>194.891310</td>\n",
       "      <td>606.347851</td>\n",
       "      <td>0.488698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.276000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.706000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.540000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.385000</td>\n",
       "      <td>9.752000</td>\n",
       "      <td>4.081000</td>\n",
       "      <td>32.478000</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.829000</td>\n",
       "      <td>1102.500000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>15841.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "count     4601.000000        4601.000000    4601.000000   4601.000000   \n",
       "mean         0.104553           0.213015       0.280656      0.064769   \n",
       "std          0.305358           1.290575       0.504143      1.392893   \n",
       "min          0.000000           0.000000       0.000000      0.000000   \n",
       "25%          0.000000           0.000000       0.000000      0.000000   \n",
       "50%          0.000000           0.000000       0.000000      0.000000   \n",
       "75%          0.000000           0.000000       0.420000      0.000000   \n",
       "max          4.540000          14.280000       5.100000     43.000000   \n",
       "\n",
       "       word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "count    4601.000000     4601.000000       4601.000000         4601.000000   \n",
       "mean        0.312223        0.095901          0.114208            0.105295   \n",
       "std         0.672513        0.273824          0.391441            0.401071   \n",
       "min         0.000000        0.000000          0.000000            0.000000   \n",
       "25%         0.000000        0.000000          0.000000            0.000000   \n",
       "50%         0.000000        0.000000          0.000000            0.000000   \n",
       "75%         0.380000        0.000000          0.000000            0.000000   \n",
       "max        10.000000        5.880000          7.270000           11.110000   \n",
       "\n",
       "       word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "count      4601.000000     4601.000000  ...  4601.000000  4601.000000   \n",
       "mean          0.090067        0.239413  ...     0.038575     0.139030   \n",
       "std           0.278616        0.644755  ...     0.243471     0.270355   \n",
       "min           0.000000        0.000000  ...     0.000000     0.000000   \n",
       "25%           0.000000        0.000000  ...     0.000000     0.000000   \n",
       "50%           0.000000        0.000000  ...     0.000000     0.065000   \n",
       "75%           0.000000        0.160000  ...     0.000000     0.188000   \n",
       "max           5.260000       18.180000  ...     4.385000     9.752000   \n",
       "\n",
       "       char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.016976     0.269071     0.075811     0.044238   \n",
       "std       0.109394     0.815672     0.245882     0.429342   \n",
       "min       0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.315000     0.052000     0.000000   \n",
       "max       4.081000    32.478000     6.003000    19.829000   \n",
       "\n",
       "       capital_run_length_average  capital_run_length_longest  \\\n",
       "count                 4601.000000                 4601.000000   \n",
       "mean                     5.191515                   52.172789   \n",
       "std                     31.729449                  194.891310   \n",
       "min                      1.000000                    1.000000   \n",
       "25%                      1.588000                    6.000000   \n",
       "50%                      2.276000                   15.000000   \n",
       "75%                      3.706000                   43.000000   \n",
       "max                   1102.500000                 9989.000000   \n",
       "\n",
       "       capital_run_length_total         spam  \n",
       "count               4601.000000  4601.000000  \n",
       "mean                 283.289285     0.394045  \n",
       "std                  606.347851     0.488698  \n",
       "min                    1.000000     0.000000  \n",
       "25%                   35.000000     0.000000  \n",
       "50%                   95.000000     0.000000  \n",
       "75%                  266.000000     1.000000  \n",
       "max                15841.000000     1.000000  \n",
       "\n",
       "[8 rows x 58 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_file.describe()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "MLA2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
